{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13738990,
          "sourceType": "datasetVersion",
          "datasetId": 8741823
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "vwtH-myLB2BZ"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:01.336173Z",
          "iopub.execute_input": "2025-11-15T10:14:01.336849Z",
          "iopub.status.idle": "2025-11-15T10:14:01.340787Z",
          "shell.execute_reply.started": "2025-11-15T10:14:01.336819Z",
          "shell.execute_reply": "2025-11-15T10:14:01.340136Z"
        },
        "id": "Dj9Ixf1nB2Bd"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(text):\n",
        "    with open(text, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "    return data\n",
        "\n",
        "text = load_dataset(\"/content/Pride_and_Prejudice-Jane_Austen.txt\")\n",
        "print(\"Number of Characters = \", len(text))\n",
        "print(text[:500])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:04.193128Z",
          "iopub.execute_input": "2025-11-15T10:14:04.193709Z",
          "iopub.status.idle": "2025-11-15T10:14:04.217430Z",
          "shell.execute_reply.started": "2025-11-15T10:14:04.193688Z",
          "shell.execute_reply": "2025-11-15T10:14:04.216882Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd4lK38qB2Be",
        "outputId": "8923ef59-c687-4ff3-e47d-039c0c4aae2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Characters =  711331\n",
            "The Project Gutenberg eBook, Pride and Prejudice, by Jane Austen, Edited\n",
            "by R. W. (Robert William) Chapman\n",
            "\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use it under the terms of the Project Gutenberg License included\n",
            "with this eBook or online at www.gutenberg.org\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Title: Pride and Prejudice\n",
            "\n",
            "\n",
            "Author: Jane Austen\n",
            "\n",
            "Editor: R. W. (Robert William) Chapman\n",
            "\n",
            "Release Date: May 9, 2013  [eBook #42671]\n",
            "\n",
            "Lang\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9.,!?;:'\\\" \\n]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return list(text)\n",
        "\n",
        "tokens = tokenize(text)\n",
        "print(\"Total tokens = \", len(tokens))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:07.673004Z",
          "iopub.execute_input": "2025-11-15T10:14:07.673574Z",
          "iopub.status.idle": "2025-11-15T10:14:07.741416Z",
          "shell.execute_reply.started": "2025-11-15T10:14:07.673551Z",
          "shell.execute_reply": "2025-11-15T10:14:07.740722Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cynXlGGnB2Bg",
        "outputId": "f89c0689-4cc5-48a6-9224-3c56c88bb6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens =  701900\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(tokens):\n",
        "    counter = Counter(tokens)\n",
        "    vocabulary = set(counter.keys())\n",
        "    stoi = {}\n",
        "    itos = {}\n",
        "\n",
        "    for i, token in enumerate(sorted(vocabulary)):\n",
        "        stoi[token] = i+2\n",
        "    stoi[\"<pad>\"] = 0\n",
        "    stoi[\"<unk>\"] = 1\n",
        "\n",
        "    for s, i in stoi.items():\n",
        "        itos[i] = s\n",
        "\n",
        "    return stoi, itos\n",
        "\n",
        "stoi, itos = build_vocabulary(tokens)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:11.368074Z",
          "iopub.execute_input": "2025-11-15T10:14:11.368346Z",
          "iopub.status.idle": "2025-11-15T10:14:11.406749Z",
          "shell.execute_reply.started": "2025-11-15T10:14:11.368326Z",
          "shell.execute_reply": "2025-11-15T10:14:11.406002Z"
        },
        "id": "nFiFLTTjB2Bh"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "def numericalize(tokens, stoi):\n",
        "    indices = []\n",
        "    for t in tokens:\n",
        "        indices.append(stoi.get(t, stoi[\"<unk>\"]))\n",
        "    return indices\n",
        "\n",
        "ids = numericalize(tokens, stoi)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:20.247708Z",
          "iopub.execute_input": "2025-11-15T10:14:20.248006Z",
          "iopub.status.idle": "2025-11-15T10:14:20.306291Z",
          "shell.execute_reply.started": "2025-11-15T10:14:20.247985Z",
          "shell.execute_reply": "2025-11-15T10:14:20.305765Z"
        },
        "id": "BNSyEWZtB2Bk"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(ids, val_ratio):\n",
        "    split = int(len(ids) * (1-val_ratio))\n",
        "    return ids[:split], ids[split:]\n",
        "\n",
        "train_ids, val_ids = train_val_split(ids, val_ratio=0.3)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:23.127372Z",
          "iopub.execute_input": "2025-11-15T10:14:23.127987Z",
          "iopub.status.idle": "2025-11-15T10:14:23.136799Z",
          "shell.execute_reply.started": "2025-11-15T10:14:23.127963Z",
          "shell.execute_reply": "2025-11-15T10:14:23.136077Z"
        },
        "id": "FOztivAJB2Bl"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "class LMDataset(Dataset):\n",
        "    def __init__(self, token_ids, seq_len):\n",
        "        self.token_ids = token_ids\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)-self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.token_ids[idx:idx+self.seq_len])\n",
        "        y = torch.tensor(self.token_ids[idx+1:idx+self.seq_len+1])\n",
        "\n",
        "        return x,y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:26.548002Z",
          "iopub.execute_input": "2025-11-15T10:14:26.548251Z",
          "iopub.status.idle": "2025-11-15T10:14:26.553185Z",
          "shell.execute_reply.started": "2025-11-15T10:14:26.548234Z",
          "shell.execute_reply": "2025-11-15T10:14:26.552265Z"
        },
        "id": "kMxcVwGEB2Bl"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(token_ids, seq_len, batch_size, shuffle):\n",
        "    dataset = LMDataset(token_ids, seq_len)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return loader\n",
        "seq_len = 64\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = data_loader(train_ids, seq_len, batch_size, shuffle=True)\n",
        "val_loader = data_loader(val_ids, seq_len, batch_size, shuffle=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:31.199109Z",
          "iopub.execute_input": "2025-11-15T10:14:31.199639Z",
          "iopub.status.idle": "2025-11-15T10:14:31.204088Z",
          "shell.execute_reply.started": "2025-11-15T10:14:31.199614Z",
          "shell.execute_reply": "2025-11-15T10:14:31.203320Z"
        },
        "id": "QYpyJJy0B2Bm"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(input_dim, 4 * hidden_dim)*0.1)\n",
        "        self.U = nn.Parameter(torch.randn(hidden_dim, 4 * hidden_dim)*0.1)\n",
        "        self.b = nn.Parameter(torch.zeros(4 * hidden_dim))\n",
        "\n",
        "        for W in [self.W, self.U]:\n",
        "            nn.init.xavier_uniform_(W)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        hidden_outputs, cell_outputs = [], []\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "            gates = x_t @ self.W + h @ self.U + self.b\n",
        "            forget_gate, input_gate, output_gate, activations = gates.chunk(4, dim=1)\n",
        "            forget_gate = torch.sigmoid(forget_gate)\n",
        "            input_gate = torch.sigmoid(input_gate)\n",
        "            output_gate = torch.sigmoid(output_gate)\n",
        "            activations = torch.tanh(activations)\n",
        "\n",
        "            c = (forget_gate * c) + (input_gate * activations)\n",
        "            h = torch.tanh(c) * output_gate\n",
        "\n",
        "            hidden_outputs.append(h.unsqueeze(1))\n",
        "            cell_outputs.append(c.unsqueeze(1))\n",
        "\n",
        "        hidden_outputs = torch.cat(hidden_outputs, dim=1)\n",
        "        cell_outputs = torch.cat(cell_outputs, dim=1)\n",
        "        return hidden_outputs, cell_outputs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:34.681148Z",
          "iopub.execute_input": "2025-11-15T10:14:34.681419Z",
          "iopub.status.idle": "2025-11-15T10:14:34.696820Z",
          "shell.execute_reply.started": "2025-11-15T10:14:34.681400Z",
          "shell.execute_reply": "2025-11-15T10:14:34.696165Z"
        },
        "id": "RTHTCqAgB2Bm"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
        "        super(MultiLayerLSTM, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(LSTMCell(input_dim, hidden_dim))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.layers.append(LSTMCell(hidden_dim, hidden_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.linear = nn.Linear(hidden_dim, input_dim)\n",
        "        nn.init.xavier_normal_(self.linear.weight.data)\n",
        "        self.linear.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        hidden, cell = h\n",
        "        output = x\n",
        "        new_hidden, new_cell = [], []\n",
        "\n",
        "        for layer_idx, layer in enumerate(self.layers):\n",
        "            h_t, c_t = hidden[layer_idx], cell[layer_idx]\n",
        "            output, cells = layer(output, h_t, c_t)\n",
        "            new_hidden.append(output[:, -1].unsqueeze(0))\n",
        "            new_cell.append(cells[:, -1].unsqueeze(0))\n",
        "            output = self.dropout(output)\n",
        "        return output, (torch.cat(new_hidden, dim=0), torch.cat(new_cell, dim=0))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:38.177388Z",
          "iopub.execute_input": "2025-11-15T10:14:38.177671Z",
          "iopub.status.idle": "2025-11-15T10:14:38.184712Z",
          "shell.execute_reply.started": "2025-11-15T10:14:38.177651Z",
          "shell.execute_reply": "2025-11-15T10:14:38.183978Z"
        },
        "id": "H_WEejdVB2Bn"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocabulary_size, embed_dim, hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(vocabulary_size, embed_dim)\n",
        "\n",
        "        self.LSTM = MultiLayerLSTM(\n",
        "            input_dim=embed_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, vocabulary_size)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return (h0, c0)\n",
        "\n",
        "    def count_parameters(self):\n",
        "        total = 0\n",
        "        for p in self.parameters():\n",
        "            if p.requires_grad:\n",
        "                total += p.numel()\n",
        "        return total\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        batch_size = x.size(0)\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(batch_size, x.device)\n",
        "        x = self.embedding(x)\n",
        "        lstm_output, hidden = self.LSTM(x, hidden)\n",
        "        logits = self.fc(lstm_output)\n",
        "        return logits, hidden"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:42.000567Z",
          "iopub.execute_input": "2025-11-15T10:14:42.000865Z",
          "iopub.status.idle": "2025-11-15T10:14:42.008239Z",
          "shell.execute_reply.started": "2025-11-15T10:14:42.000843Z",
          "shell.execute_reply": "2025-11-15T10:14:42.007405Z"
        },
        "id": "lpBLiUTVB2Bo"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(stoi)\n",
        "\n",
        "model = LSTMModel(\n",
        "    vocabulary_size=vocabulary_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "print(\"Model Parameters = \", model.count_parameters)\n",
        "batch = next(iter(train_loader))[0]   # get x only\n",
        "logits, hidden = model(batch)\n",
        "\n",
        "print(\"Logits shape:\", logits.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:14:53.018926Z",
          "iopub.execute_input": "2025-11-15T10:14:53.019166Z",
          "iopub.status.idle": "2025-11-15T10:14:53.374644Z",
          "shell.execute_reply.started": "2025-11-15T10:14:53.019150Z",
          "shell.execute_reply": "2025-11-15T10:14:53.373957Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-8_N9HcB2Bp",
        "outputId": "87040a98-5d20-408b-81cd-fb4fd2c67971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Parameters =  <bound method LSTMModel.count_parameters of LSTMModel(\n",
            "  (embedding): Embedding(47, 128)\n",
            "  (LSTM): MultiLayerLSTM(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x LSTMCell()\n",
            "    )\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=47, bias=True)\n",
            ")>\n",
            "Logits shape: torch.Size([32, 64, 47])\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device, clip=1.0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    hidden = None  # LSTM hidden state\n",
        "\n",
        "    tqdm_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for x, y in tqdm_bar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if hidden is not None and hidden[0].size(1) != x.size(0):\n",
        "            hidden = None\n",
        "\n",
        "        # Forward pass\n",
        "        logits, hidden = model(x, hidden)\n",
        "\n",
        "        # Detach hidden state to prevent gradients from flowing through entire history\n",
        "        if hidden is not None:\n",
        "            if isinstance(hidden, tuple):  # LSTM\n",
        "                hidden = tuple(h.detach() for h in hidden)\n",
        "            else:  # GRU\n",
        "                hidden = tuple(hidden.detach() for h in hidden)\n",
        "\n",
        "        # Reshape for cross-entropy\n",
        "        loss = criterion(\n",
        "            logits.view(-1, logits.size(-1)),  # (B*L, vocab)\n",
        "            y.view(-1)                         # (B*L)\n",
        "        )\n",
        "\n",
        "        # Backprop\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (tqdm_bar.n + 1)\n",
        "        tqdm_bar.set_postfix(loss=f\"{avg_loss:.2f}\")\n",
        "\n",
        "    return avg_loss"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:15:11.316107Z",
          "iopub.execute_input": "2025-11-15T10:15:11.316504Z",
          "iopub.status.idle": "2025-11-15T10:15:11.323211Z",
          "shell.execute_reply.started": "2025-11-15T10:15:11.316479Z",
          "shell.execute_reply": "2025-11-15T10:15:11.322385Z"
        },
        "id": "T45WNUKAB2Bp"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            if hidden is not None and hidden[0].size(1) != x.size(0):\n",
        "                hidden = None\n",
        "\n",
        "            logits, hidden = model(x, hidden)\n",
        "\n",
        "            # Detach hidden\n",
        "            if hidden is not None:\n",
        "                if isinstance(hidden, tuple):\n",
        "                    hidden = tuple(h.detach() for h in hidden)\n",
        "                else:\n",
        "                    hidden = tuple(hidden.detach() for h in hidden)\n",
        "\n",
        "            loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:15:14.996218Z",
          "iopub.execute_input": "2025-11-15T10:15:14.996943Z",
          "iopub.status.idle": "2025-11-15T10:15:15.002685Z",
          "shell.execute_reply.started": "2025-11-15T10:15:14.996919Z",
          "shell.execute_reply": "2025-11-15T10:15:15.001927Z"
        },
        "id": "nCtVr4_tB2Bq"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    perplexities = []\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
        "\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss   = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        perplexities.append(math.exp(val_loss))\n",
        "\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
        "        print(f\"  Val Perplexity: {math.exp(val_loss):.4f}\")\n",
        "\n",
        "    return train_losses, val_losses, perplexities\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:15:18.528449Z",
          "iopub.execute_input": "2025-11-15T10:15:18.529061Z",
          "iopub.status.idle": "2025-11-15T10:15:18.534444Z",
          "shell.execute_reply.started": "2025-11-15T10:15:18.529037Z",
          "shell.execute_reply": "2025-11-15T10:15:18.533725Z"
        },
        "id": "1lYWB0EjB2Br"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = LSTMModel(\n",
        "    vocabulary_size=len(stoi),\n",
        "    embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "train_losses, val_losses, perplexities = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=10,\n",
        "    lr=0.001,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "torch.save(model.state_dict(), \"best_model.pt\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T10:15:26.941789Z",
          "iopub.execute_input": "2025-11-15T10:15:26.942318Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCn0mpdSB2Br",
        "outputId": "6eac555a-6661-4dc5-84a1-0d763f7eb565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 1.1362\n",
            "  Val Loss:   1.2568\n",
            "  Val Perplexity: 3.5142\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 0.9903\n",
            "  Val Loss:   1.2838\n",
            "  Val Perplexity: 3.6103\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 0.9616\n",
            "  Val Loss:   1.2990\n",
            "  Val Perplexity: 3.6657\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 0.9475\n",
            "  Val Loss:   1.3031\n",
            "  Val Perplexity: 3.6806\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 0.9384\n",
            "  Val Loss:   1.3117\n",
            "  Val Perplexity: 3.7124\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 0.9315\n",
            "  Val Loss:   1.3188\n",
            "  Val Perplexity: 3.7390\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 0.9255\n",
            "  Val Loss:   1.3220\n",
            "  Val Perplexity: 3.7509\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 0.9207\n",
            "  Val Loss:   1.3242\n",
            "  Val Perplexity: 3.7593\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train Loss: 0.9159\n",
            "  Val Loss:   1.3232\n",
            "  Val Perplexity: 3.7556\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:  71%|███████   | 4666/6579 [01:59<01:02, 30.74it/s]"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"plots\", exist_ok=True)\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='o')\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"plots/loss_curve_128x256.png\")\n",
        "plt.show()\n",
        "\n",
        "# 2️⃣ Perplexity Curve\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, perplexities, label=\"Validation Perplexity\", color='purple', marker='o')\n",
        "plt.title(\"Validation Perplexity over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.yscale(\"log\")  # Optional: log-scale for better visualization\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"plots/perplexity_curve_128x256.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Plots saved.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-15T06:31:33.841235Z",
          "iopub.execute_input": "2025-11-15T06:31:33.841976Z",
          "iopub.status.idle": "2025-11-15T06:31:34.625190Z",
          "shell.execute_reply.started": "2025-11-15T06:31:33.841934Z",
          "shell.execute_reply": "2025-11-15T06:31:34.624394Z"
        },
        "id": "Q0wFo4rCB2Bs"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}